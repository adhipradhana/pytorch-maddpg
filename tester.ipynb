{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battle Royale Environment Trainer\n",
    "This notebook is for training Battle Royale agents. MADDPG is used for training the agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "/Users/adhipradhana/anaconda3/envs/unity-battle-royale/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "print(sys.executable)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs:Connected new brain:\n",
      "PlayerBrain?team=0\n",
      "INFO:gym_unity:2 agents within environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UnityEnv instance>\n"
     ]
    }
   ],
   "source": [
    "# Environment name\n",
    "# Remember to put battle royale environment configuration within the config folder\n",
    "env_name = \"environment/battle-royale\"\n",
    "\n",
    "env = UnityEnv(env_name, worker_id=0, use_visual=False, multiagent=True)\n",
    "\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Algorithm Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import visdom\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.MADDPG import MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1000\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "n_agents = env.number_agents\n",
    "n_episode = 100\n",
    "max_steps = 100\n",
    "buffer_capacity = 1000000\n",
    "batch_size = 1000\n",
    "episodes_before_train = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup seed\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "maddpg = MADDPG(n_agents, n_states, n_actions, batch_size, buffer_capacity, episodes_before_train, use_approx=True)\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if maddpg.use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var 0: 0.5591438069865847\n",
      "Var 1: 0.4028315037868575\n"
     ]
    }
   ],
   "source": [
    "## Model from experiment 4x4\n",
    "# path_normal = os.path.join(os.getcwd(), 'checkpoint', '4x4 Normal', 'Time_2020-04-20_07:09:21.273684_NAgent_2_Episode_5000.pth')\n",
    "# path_static = os.path.join(os.getcwd(), 'checkpoint', '4x4 Static', 'Time_2020-04-20_07:06:38.159288_NAgent_2_Episode_5000.pth')\n",
    "# path_aggressive = os.path.join(os.getcwd(), 'checkpoint', '4x4 Aggressive', 'Time_2020-04-22_00:57:16.521466_NAgent_2_Episode_5000.pth')\n",
    "path_passive = os.path.join(os.getcwd(), 'checkpoint', '4x4 Passive', 'Time_2020-04-21_02:53:27.519503_NAgent_2_Episode_5000.pth')\n",
    "\n",
    "## Model from experiment 8x10\n",
    "# path_aggressive = os.path.join(os.getcwd(), 'checkpoint', '8x10 Aggressive', 'Time_2020-04-24_10:41:51.445399_NAgent_2_Episode_5000.pth')\n",
    "path_passive_new = os.path.join(os.getcwd(), 'checkpoint', '8x10 Passive', 'Time_2020-04-23_04:04:35.951300_NAgent_2_Episode_5000.pth')\n",
    "\n",
    "## Model from experiment 6x6\n",
    "# path_aggressive_new = os.path.join(os.getcwd(), 'checkpoint', '6x6 Aggressive', 'Time_2020-04-26_05:30:22.037620_NAgent_2_Episode_5000.pth')\n",
    "\n",
    "## Model from new experiment \n",
    "# path_normal_new = os.path.join(os.getcwd(), 'checkpoint', 'New Environment', '6x6 Normal', 'Time_2020-04-29_04:43:47.774365_NAgent_2_Episode_5000.pth')\n",
    "\n",
    "# maddpg.load(path=path_normal, map_location='cpu')\n",
    "maddpg.load_agent(path=path_passive, agent_number=0, model_number=1, map_location='cpu')\n",
    "maddpg.load_agent(path=path_passive_new, agent_number=1, model_number=1, map_location='cpu')\n",
    "# maddpg.load_agent(path=path_aggressive, agent_number=0, model_number=0, map_location='cpu')\n",
    "# maddpg.load_agent(path=path_passive, agent_number=1, model_number=1, map_location='cpu')\n",
    "\n",
    "for i in range(maddpg.n_agents):\n",
    "    print(\"Var {}: {}\".format(i, maddpg.var[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "Episode: 0, reward = -0.07499996572732925\n",
      "[True, True]\n",
      "Episode: 1, reward = 2.2750000953674316\n",
      "[True, True]\n",
      "Episode: 2, reward = 2.1575000286102295\n",
      "[True, True]\n",
      "Episode: 3, reward = 2.1649999618530273\n",
      "Episode: 4, reward = -0.1799999475479126\n",
      "[True, True]\n",
      "Episode: 5, reward = 2.819999933242798\n",
      "Episode: 6, reward = -0.38499999046325684\n",
      "[True, True]\n",
      "Episode: 7, reward = 2.0174999237060547\n",
      "[True, True]\n",
      "Episode: 8, reward = 2.1649999618530273\n",
      "Episode: 9, reward = -0.1799999624490738\n",
      "[True, True]\n",
      "Episode: 10, reward = 1.902500033378601\n",
      "[True, True]\n",
      "Episode: 11, reward = 2.2100000381469727\n",
      "[True, True]\n",
      "Episode: 12, reward = 2.0975000858306885\n",
      "Episode: 13, reward = -0.3199998736381531\n",
      "Episode: 14, reward = -0.14499995112419128\n",
      "[True, True]\n",
      "Episode: 15, reward = 2.202500104904175\n",
      "Episode: 16, reward = -0.2074998915195465\n",
      "[True, True]\n",
      "Episode: 17, reward = 2.2225000858306885\n",
      "[True, True]\n",
      "Episode: 18, reward = 1.8750001192092896\n",
      "[True, True]\n",
      "Episode: 19, reward = 2.2174999713897705\n",
      "Episode: 20, reward = -0.1850000023841858\n",
      "[True, True]\n",
      "Episode: 21, reward = 2.052500009536743\n",
      "[True, True]\n",
      "Episode: 22, reward = 2.1524999141693115\n",
      "[True, True]\n",
      "Episode: 23, reward = 2.1875\n",
      "[True, True]\n",
      "Episode: 24, reward = 2.930000066757202\n",
      "[True, True]\n",
      "Episode: 25, reward = 2.194999933242798\n",
      "Episode: 26, reward = -0.26499998569488525\n",
      "[True, True]\n",
      "Episode: 27, reward = 1.9975000619888306\n",
      "Episode: 28, reward = -0.3125\n",
      "[True, True]\n",
      "Episode: 29, reward = 2.1500000953674316\n",
      "[True, True]\n",
      "Episode: 30, reward = 2.140000104904175\n",
      "[True, True]\n",
      "Episode: 31, reward = 2.2774999141693115\n",
      "Episode: 32, reward = -0.3549998700618744\n",
      "[True, True]\n",
      "Episode: 33, reward = 2.1725001335144043\n",
      "[True, True]\n",
      "Episode: 34, reward = 2.1549999713897705\n",
      "Episode: 35, reward = -0.23249991238117218\n",
      "Episode: 36, reward = -0.17749997973442078\n",
      "[True, True]\n",
      "Episode: 37, reward = 1.9725000858306885\n",
      "[True, True]\n",
      "Episode: 38, reward = 2.065000057220459\n",
      "[True, True]\n",
      "Episode: 39, reward = 2.2049999237060547\n",
      "[True, True]\n",
      "Episode: 40, reward = 2.005000114440918\n",
      "[True, True]\n",
      "Episode: 41, reward = 1.8675001859664917\n",
      "Episode: 42, reward = -0.2674998939037323\n",
      "[True, True]\n",
      "Episode: 43, reward = 2.192500114440918\n",
      "[True, True]\n",
      "Episode: 44, reward = 2.174999952316284\n",
      "Episode: 45, reward = -0.2399999499320984\n",
      "[True, True]\n",
      "Episode: 46, reward = 2.1875\n",
      "Episode: 47, reward = -0.22749993205070496\n",
      "Episode: 48, reward = -0.4049999713897705\n",
      "[True, True]\n",
      "Episode: 49, reward = 2.2024998664855957\n",
      "Episode: 50, reward = -0.3574999272823334\n",
      "Episode: 51, reward = -0.25749993324279785\n",
      "[True, True]\n",
      "Episode: 52, reward = 2.067500114440918\n",
      "[True, True]\n",
      "Episode: 53, reward = 2.122499942779541\n",
      "[True, True]\n",
      "Episode: 54, reward = 2.070000171661377\n",
      "Episode: 55, reward = -0.15999996662139893\n",
      "[True, True]\n",
      "Episode: 56, reward = 2.2225000858306885\n",
      "Episode: 57, reward = -0.22249995172023773\n",
      "[True, True]\n",
      "Episode: 58, reward = 2.122499942779541\n",
      "[True, True]\n",
      "Episode: 59, reward = 2.127500057220459\n",
      "[True, True]\n",
      "Episode: 60, reward = 2.0225000381469727\n",
      "Episode: 61, reward = -0.19749996066093445\n",
      "[True, True]\n",
      "Episode: 62, reward = 2.072499990463257\n",
      "[True, True]\n",
      "Episode: 63, reward = 2.132499933242798\n",
      "Episode: 64, reward = -0.18249990046024323\n",
      "[True, True]\n",
      "Episode: 65, reward = 2.122499942779541\n",
      "[True, True]\n",
      "Episode: 66, reward = 2.072499990463257\n",
      "Episode: 67, reward = -0.13999995589256287\n",
      "Episode: 68, reward = -0.42000001668930054\n",
      "[True, True]\n",
      "Episode: 69, reward = 2.067499876022339\n",
      "Episode: 70, reward = -0.19499997794628143\n",
      "[True, True]\n",
      "Episode: 71, reward = 2.0299999713897705\n",
      "[True, True]\n",
      "Episode: 72, reward = 2.112499952316284\n",
      "[True, True]\n",
      "Episode: 73, reward = 1.9700000286102295\n",
      "[True, True]\n",
      "Episode: 74, reward = 2.1024999618530273\n",
      "[True, True]\n",
      "Episode: 75, reward = 2.0450000762939453\n",
      "[True, True]\n",
      "Episode: 76, reward = 2.0199999809265137\n",
      "[True, True]\n",
      "Episode: 77, reward = 2.1050000190734863\n",
      "[True, True]\n",
      "Episode: 78, reward = 2.177500009536743\n",
      "[True, True]\n",
      "Episode: 79, reward = 2.1524999141693115\n",
      "Episode: 80, reward = -0.10749997198581696\n",
      "Episode: 81, reward = -0.2699999213218689\n",
      "[True, True]\n",
      "Episode: 82, reward = 2.1424999237060547\n",
      "Episode: 83, reward = -0.16749995946884155\n",
      "Episode: 84, reward = -0.21999993920326233\n",
      "[True, True]\n",
      "Episode: 85, reward = 2.182500123977661\n",
      "Episode: 86, reward = -0.2099999487400055\n",
      "[True, True]\n",
      "Episode: 87, reward = 2.190000057220459\n",
      "[True, True]\n",
      "Episode: 88, reward = 2.129999876022339\n",
      "[True, True]\n",
      "Episode: 89, reward = 2.0274999141693115\n",
      "[True, True]\n",
      "Episode: 90, reward = 2.057500123977661\n",
      "[True, True]\n",
      "Episode: 91, reward = 2.127500057220459\n",
      "[True, True]\n",
      "Episode: 92, reward = 2.190000057220459\n",
      "[True, True]\n",
      "Episode: 93, reward = 2.0299999713897705\n",
      "[True, True]\n",
      "Episode: 94, reward = 2.2350001335144043\n",
      "Episode: 95, reward = -0.05249999836087227\n",
      "[True, True]\n",
      "Episode: 96, reward = 2.122499942779541\n",
      "[True, True]\n",
      "Episode: 97, reward = 2.1600000858306885\n",
      "Episode: 98, reward = -0.43999987840652466\n",
      "[True, True]\n",
      "Episode: 99, reward = 2.115000009536743\n",
      "Training begins...\n",
      "MADDPG on Battle Royale\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing model...\")\n",
    "for i_episode in range(n_episode):\n",
    "    # reset environment\n",
    "    obs = env.reset()\n",
    "    obs = np.stack(obs)\n",
    "    \n",
    "    # convert observation to tensor\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    rr = np.zeros((n_agents,))\n",
    "    for i_step in range(max_steps):\n",
    "        obs = obs.type(FloatTensor)\n",
    "        actions = maddpg.select_action(obs).data.cpu()\n",
    "        \n",
    "        # for debug\n",
    "#         actions[1] = torch.zeros(n_actions)\n",
    "#         actions[1][4] = -1\n",
    "        \n",
    "        actions_list = actions.tolist()\n",
    "        \n",
    "        obs_, reward, done, _ = env.step(actions_list)\n",
    "        \n",
    "        reward = torch.FloatTensor(reward).type(FloatTensor)\n",
    "        obs_ = np.stack(obs_)\n",
    "        obs_ = torch.from_numpy(obs_).float()\n",
    "        if i_step != max_steps - 1:\n",
    "            next_obs = obs_\n",
    "        else:\n",
    "            next_obs = None\n",
    "\n",
    "        total_reward += reward.sum()   \n",
    "        rr += reward.cpu().numpy()\n",
    "        obs = next_obs\n",
    "\n",
    "        # check if done\n",
    "        if True in done:\n",
    "            print(done)\n",
    "            break\n",
    "\n",
    "    maddpg.episode_done += 1\n",
    "    print(\"Episode: {}, reward = {}\".format(i_episode, total_reward))\n",
    "  \n",
    "    if maddpg.episode_done == maddpg.episodes_before_train:\n",
    "        print(\"Training begins...\")\n",
    "        print(\"MADDPG on Battle Royale\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs:Environment shut down with return code 0.\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity-battle-royale",
   "language": "python",
   "name": "unity-battle-royale"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
